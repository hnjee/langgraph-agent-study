{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddcdd655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "small_llm = ChatOpenAI(model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c3d892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool \n",
    "def add(a: int, b:int) -> int:\n",
    "    \"\"\"숫자 a와 b를 더합니다.\"\"\" \n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b:int) -> int:\n",
    "    \"\"\"숫자 a와 b를 곱합니다.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a379138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "web_search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "#web_search_tool.invoke(\"오바마가 태어난 곳의 화폐 단위는?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a724968a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (token_file: 'Optional[str]' = None, client_sercret_file: 'Optional[str]' = None, service_account_file: 'Optional[str]' = None, scopes: 'Optional[List[str]]' = None, use_domain_wide: 'bool' = False, delegated_user: 'Optional[str]' = None) -> 'Credentials'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "from langchain_google_community.gmail.utils import get_gmail_credentials\n",
    "\n",
    "inspect.signature(get_gmail_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a7c1934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/84bkz1z16w5c0lv50z7fjm_40000gn/T/ipykernel_72262/1023507223.py:9: DeprecationWarning: get_gmail_credentials is deprecated and will be removed in a future version.Use get_google_credentials instead.\n",
      "  credentials = get_gmail_credentials(\n",
      "/var/folders/h2/84bkz1z16w5c0lv50z7fjm_40000gn/T/ipykernel_72262/1023507223.py:15: DeprecationWarning: build_resource_service is deprecated and will be removed in a future version.Use build_gmail_service instead.\n",
      "  api_resource = build_resource_service(credentials=credentials)\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_community import GmailToolkit\n",
    "from langchain_google_community.gmail.utils import (\n",
    "    build_resource_service,\n",
    "    get_gmail_credentials,\n",
    ")\n",
    "\n",
    "# Can review scopes here https://developers.google.com/gmail/api/auth/scopes\n",
    "# For instance, readonly scope is 'https://www.googleapis.com/auth/gmail.readonly'\n",
    "credentials = get_gmail_credentials(\n",
    "    token_file=\"../google/token.json\", #나중에 만들어지는 파일 \n",
    "    scopes=[\"https://mail.google.com/\"],\n",
    "    client_sercret_file=\"../google/credentials.json\",\n",
    ")\n",
    "\n",
    "api_resource = build_resource_service(credentials=credentials)\n",
    "gmail_toolkit = GmailToolkit(api_resource=api_resource)\n",
    "gmail_tool_list = gmail_toolkit.get_tools()\n",
    "\n",
    "#app publish, gmail API 승인 필요 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da660838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "\n",
    "arxiv_tool = ArxivQueryRun(api_wrapper=ArxivAPIWrapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a1ad6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name = 'real_estate_tax',\n",
    "    persist_directory='../real_estate_tax_collection'\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name='real_estate_tax_retriever',\n",
    "    description=\"2024년 12월까지의 부동산 세금(취득세, 보유세, 양도소득세 등) 정보를 포함하고 있습니다. 부동산 세금 관련 법령이나 계산 방식에 대한 질문이 들어오면 이 도구를 사용하세요.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ad9735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "#함수 객체 리스트 정의 \n",
    "tool_list = [add, multiply, web_search_tool, arxiv_tool, retriever_tool] + gmail_tool_list\n",
    "\n",
    "#모델에 도구 설명서 전달 \n",
    "#llm_with_tools = small_llm.bind_tools(tool_list)\n",
    "llm_with_tools = llm.bind_tools(tool_list)\n",
    "\n",
    "#함수 객체 리스트를 전달받아, llm의 도구 호출 지시에 따라 실제로 파이썬 함수를 실행하는 노드를 생성\n",
    "tool_node = ToolNode(tool_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390cc2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "# graph_builder = StateGraph(AgentState)\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40623a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"\n",
    "    에이전트 함수는 주어진 상태에서 메시지를 가져와\n",
    "    LLM과 도구를 사용하여 응답 메시지 (AIMessage)를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): 메시지 상태를 포함하는 state.\n",
    "\n",
    "    Returns:\n",
    "        MessagesState: 응답 메시지를 포함하는 새로운 state.\n",
    "    \"\"\"\n",
    "    # 상태에서 메시지 리스트를 추출합니다.\n",
    "    messages = state['messages']\n",
    "    \n",
    "    # LLM과 도구를 사용하여 메시지를 처리하고 응답을 생성합니다.\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # 응답 메시지를 새로운 상태로 반환합니다.\n",
    "    return {'messages': [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fce26a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x121b261d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_node('agent', agent)\n",
    "graph_builder.add_node('tools', tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c07d06ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x121b261d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import START, END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "graph_builder.add_edge(START, 'agent')\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    'agent',\n",
    "    tools_condition,\n",
    ")\n",
    "graph_builder.add_edge('tools', 'agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "178e1f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 1. 기억 저장소(창고) 객체를 만듭니다.\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# 2. 그래프를 컴파일할 때 이 저장소를 연결합니다.\n",
    "graph = graph_builder.compile(\n",
    "    checkpointer = checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56da74ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Attention is All You Need라는 논문을 요약해서 leehnjee@gmail.com으로 보내기 위한 초안을 작성해주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (call_fmCrkUiYVBUHUyFQX1vDSYmv)\n",
      " Call ID: call_fmCrkUiYVBUHUyFQX1vDSYmv\n",
      "  Args:\n",
      "    query: Attention is All You Need\n",
      "  create_gmail_draft (call_YKNaSMgp5c9NdTfA5MZva2RK)\n",
      " Call ID: call_YKNaSMgp5c9NdTfA5MZva2RK\n",
      "  Args:\n",
      "    message: 이 이메일은 NLP 분야에서 Transformer 구조의 기초를 설명한 논문 'Attention is All You Need'의 요약을 포함하고 있습니다. Transformer는 주로 셀프 어텐션 메커니즘을 활용하여 번역, 텍스트 생성 등의 작업에서 혁신적인 성과를 보여주었습니다. 이 논문은 구체적으로 어텐션 메커니즘이 어떠한 방식으로 작동하는지, 그리고 기존 구조들 대비 어떠한 이점이 있는지를 설명합니다.\n",
      "    to: ['leehnjee@gmail.com']\n",
      "    subject: Attention is All You Need 논문 요약\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: create_gmail_draft\n",
      "\n",
      "Draft created. Draft Id: r2712269448353513460\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I have created a draft email with the requested subject and message to summarize the paper \"Attention is All You Need.\" Here is a brief summary of the paper for inclusion in the email:\n",
      "\n",
      "---\n",
      "\n",
      "**Title:** Attention Is All You Need\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "This paper introduces the Transformer, a novel neural network architecture based on a self-attention mechanism that provides significant improvements in tasks like translation and text generation. The paper details the operation of the attention mechanism and explains how the architecture outperforms previous models.\n",
      "\n",
      "---\n",
      "\n",
      "The email draft has been successfully created and is ready to be sent to leehnjee@gmail.com. If you would like to send the email or require any further modifications, please let me know!\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "# 3. 특정 대화방을 구분할 번호표(thread_id)를 정합니다.\n",
    "config = {\n",
    "    'configurable': {\n",
    "        'thread_id': 'paper_summary_01'  # 대화방 별 고유 이름\n",
    "    }\n",
    "}\n",
    "\n",
    "query = \"Attention is All You Need라는 논문을 요약해서 leehnjee@gmail.com으로 보내기 위한 초안을 작성해주세요.\"\n",
    "\n",
    "# 4. 실행할 때 이 config를 함께 전달합니다.\n",
    "for chunk in graph.stream({'messages': [HumanMessage(query)]}, config=config, stream_mode='values'):\n",
    "    chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "816af97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "초안에 논문의 출처 url도 추가해주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (call_jfo22Ohn8k6ztv5m2o6Vkf0K)\n",
      " Call ID: call_jfo22Ohn8k6ztv5m2o6Vkf0K\n",
      "  Args:\n",
      "    query: Attention is All You Need site:arxiv.org\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2021-05-06\n",
      "Title: Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\n",
      "Authors: Luke Melas-Kyriazi\n",
      "Summary: The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9\\% top-1 accuracy, compared to 77.9\\% and 79.9\\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.\n",
      "\n",
      "Published: 2025-12-03\n",
      "Title: \"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme\n",
      "Authors: Anton Alyakin\n",
      "Summary: The 2017 paper ''Attention Is All You Need'' introduced the Transformer architecture-and inadvertently spawned one of machine learning's most persistent naming conventions. We analyze 717 arXiv preprints containing ''All You Need'' in their titles (2009-2025), finding exponential growth ($R^2$ > 0.994) following the original paper, with 200 titles in 2025 alone. Among papers following the canonical ''X [Is] All You Need'' structure, ''Attention'' remains the most frequently claimed necessity (28 occurrences). Situating this phenomenon within memetic theory, we argue the pattern's success reflects competitive pressures in scientific communication that increasingly favor memorability over precision. Whether this trend represents harmless academic whimsy or symptomatic sensationalism, we leave-with appropriate self-awareness-to the reader.\n",
      "\n",
      "Published: 2018-06-28\n",
      "Title: Quit When You Can: Efficient Evaluation of Ensembles with Ordering Optimization\n",
      "Authors: Serena Wang, Maya Gupta, Seungil You\n",
      "Summary: Given a classifier ensemble and a set of examples to be classified, many examples may be confidently and accurately classified after only a subset of the base models in the ensemble are evaluated. This can reduce both mean latency and CPU while maintaining the high accuracy of the original ensemble. To achieve such gains, we propose jointly optimizing a fixed evaluation order of the base models and early-stopping thresholds. Our proposed objective is a combinatorial optimization problem, but we provide a greedy algorithm that achieves a 4-approximation of the optimal solution for certain cases. For those cases, this is also the best achievable polynomial time approximation bound unless $P = NP$. Experiments on benchmark and real-world problems show that the proposed Quit When You Can (QWYC) algorithm can speed-up average evaluation time by $2$x--$4$x, and is around $1.5$x faster than prior work. QWYC's joint optimization of ordering and thresholds also performed better in experiments than various fixed orderings, including gradient boosted trees' ordering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Impersonate 'chrome_124' does not exist, using 'random'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  duckduckgo_search (call_2upxMnem4h7xTziR86GvowuB)\n",
      " Call ID: call_2upxMnem4h7xTziR86GvowuB\n",
      "  Args:\n",
      "    query: Attention is All You Need arxiv link\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: duckduckgo_search\n",
      "\n",
      "2025. 10. 21. · Abstract:We demonstrate complete functional segregation in hybrid SSM-Transformer architectures: retrieval depends exclusively on self-attention layers. 2025. 5. 29. · tuning을 하지않았음에도 좋은 성능을 확인. 참고 링크. Vaswani, A., et al. (2017). Attention Is All You Need. arXiv:1706.03762. https://arxiv.org/abs ... 2025. 4. 5. · \"Attention Is All You Need\"는 Transformer 모델을 제안한 기념비적인 논문으로, 자연어 처리 분야에 혁신을 일으킨 연구이다. 기존의 순환 신경망(RNN) 기반 모델들과 ... 2025. 8. 3. · ATTENTION IS ALL YOU NEED Original Paper by Ashish Vaswani, Noam Shazeer ... Original Paper link- https://arxiv.org/pdf/1706.03762. Table of Contents. 1 ... 2025. 12. 12. · Abstract page for arXiv paper 2512.11254: FAIR: Focused Attention Is All You Need for Generative Recommendation.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  create_gmail_draft (call_RsNfQeNyV4abOZsakdH2udq9)\n",
      " Call ID: call_RsNfQeNyV4abOZsakdH2udq9\n",
      "  Args:\n",
      "    message: 이 이메일은 NLP 분야에서 Transformer 구조의 기초를 설명한 논문 'Attention is All You Need'의 요약을 포함하고 있습니다. Transformer는 주로 셀프 어텐션 메커니즘을 활용하여 번역, 텍스트 생성 등의 작업에서 혁신적인 성과를 보여주었습니다. 이 논문은 구체적으로 어텐션 메커니즘이 어떠한 방식으로 작동하는지, 그리고 기존 구조들 대비 어떠한 이점이 있는지를 설명합니다.\n",
      "\n",
      "논문의 출처: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
      "    to: ['leehnjee@gmail.com']\n",
      "    subject: Attention is All You Need 논문 요약\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: create_gmail_draft\n",
      "\n",
      "Draft created. Draft Id: r-3203269290635407294\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've added the source URL of the paper \"Attention is All You Need\" to the email draft. Here is the updated summary with the link included:\n",
      "\n",
      "---\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "This email includes a summary of the paper 'Attention is All You Need,' which lays the foundation for the Transformer architecture in the field of NLP. The Transformer utilizes a self-attention mechanism to achieve groundbreaking results in translation, text generation, and more. The paper explains in detail how the attention mechanism works and highlights the advantages over previous architectures.\n",
      "\n",
      "**Paper Source:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
      "\n",
      "---\n",
      "\n",
      "The draft is ready to be sent to leehnjee@gmail.com. Let me know if you need further adjustments or if you're ready to send the email!\n"
     ]
    }
   ],
   "source": [
    "updated_query = \"초안에 논문의 출처 url도 추가해주세요.\"\n",
    "\n",
    "for chunk in graph.stream({'messages': [HumanMessage(updated_query)]}, config=config, stream_mode='values'): #paper_summary_01 대화방에 재질문 \n",
    "    chunk['messages'][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27cb9ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "좋습니다. 이 초안을 전송해주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  send_gmail_message (call_tshfQ7XEuwQMxSCO2CfhKXqF)\n",
      " Call ID: call_tshfQ7XEuwQMxSCO2CfhKXqF\n",
      "  Args:\n",
      "    message: 이 이메일은 NLP 분야에서 Transformer 구조의 기초를 설명한 논문 'Attention is All You Need'의 요약을 포함하고 있습니다. Transformer는 주로 셀프 어텐션 메커니즘을 활용하여 번역, 텍스트 생성 등의 작업에서 혁신적인 성과를 보여주었습니다. 이 논문은 구체적으로 어텐션 메커니즘이 어떠한 방식으로 작동하는지, 그리고 기존 구조들 대비 어떠한 이점이 있는지를 설명합니다.\n",
      "\n",
      "논문의 출처: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
      "    to: ['leehnjee@gmail.com']\n",
      "    subject: Attention is All You Need 논문 요약\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: send_gmail_message\n",
      "\n",
      "Message sent. Message Id: 19c8f923cf2cf2b4\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The email with the summary of \"Attention is All You Need\" has been successfully sent to leehnjee@gmail.com. If there's anything else you need, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "updated_query = \"좋습니다. 이 초안을 전송해주세요.\"\n",
    "\n",
    "for chunk in graph.stream({'messages': [HumanMessage(updated_query)]}, config=config, stream_mode='values'): #paper_summary_01 대화방에 재질문 \n",
    "    chunk['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1233657d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Attention is All You Need라는 논문을 요약해서 leehnjee@gmail.com으로 보내기 위한 초안을 작성해주세요.', additional_kwargs={}, response_metadata={}, id='bfb5265a-8cdb-4d72-b04a-b31b5f4576ff'),\n",
       " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 180, 'prompt_tokens': 818, 'total_tokens': 998, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DClZgfZ27yevCIsm2s6IIGPzxTQC9', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c8f91-32e9-7721-9d00-cc962cc7ae8d-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'Attention is All You Need'}, 'id': 'call_fmCrkUiYVBUHUyFQX1vDSYmv', 'type': 'tool_call'}, {'name': 'create_gmail_draft', 'args': {'message': \"이 이메일은 NLP 분야에서 Transformer 구조의 기초를 설명한 논문 'Attention is All You Need'의 요약을 포함하고 있습니다. Transformer는 주로 셀프 어텐션 메커니즘을 활용하여 번역, 텍스트 생성 등의 작업에서 혁신적인 성과를 보여주었습니다. 이 논문은 구체적으로 어텐션 메커니즘이 어떠한 방식으로 작동하는지, 그리고 기존 구조들 대비 어떠한 이점이 있는지를 설명합니다.\", 'to': ['leehnjee@gmail.com'], 'subject': 'Attention is All You Need 논문 요약'}, 'id': 'call_YKNaSMgp5c9NdTfA5MZva2RK', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 818, 'output_tokens': 180, 'total_tokens': 998, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='Published: 2021-05-06\\nTitle: Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\\nAuthors: Luke Melas-Kyriazi\\nSummary: The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9\\\\% top-1 accuracy, compared to 77.9\\\\% and 79.9\\\\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.\\n\\nPublished: 2025-12-03\\nTitle: \"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme\\nAuthors: Anton Alyakin\\nSummary: The 2017 paper \\'\\'Attention Is All You Need\\'\\' introduced the Transformer architecture-and inadvertently spawned one of machine learning\\'s most persistent naming conventions. We analyze 717 arXiv preprints containing \\'\\'All You Need\\'\\' in their titles (2009-2025), finding exponential growth ($R^2$ > 0.994) following the original paper, with 200 titles in 2025 alone. Among papers following the canonical \\'\\'X [Is] All You Need\\'\\' structure, \\'\\'Attention\\'\\' remains the most frequently claimed necessity (28 occurrences). Situating this phenomenon within memetic theory, we argue the pattern\\'s success reflects competitive pressures in scientific communication that increasingly favor memorability over precision. Whether this trend represents harmless academic whimsy or symptomatic sensationalism, we leave-with appropriate self-awareness-to the reader.\\n\\nPublished: 2018-06-28\\nTitle: Quit When You Can: Efficient Evaluation of Ensembles with Ordering Optimization\\nAuthors: Serena Wang, Maya Gupta, Seungil You\\nSummary: Given a classifier ensemble and a set of examples to be classified, many examples may be confidently and accurately classified after only a subset of the base models in the ensemble are evaluated. This can reduce both mean latency and CPU while maintaining the high accuracy of the original ensemble. To achieve such gains, we propose jointly optimizing a fixed evaluation order of the base models and early-stopping thresholds. Our proposed objective is a combinatorial optimization problem, but we provide a greedy algorithm that achieves a 4-approximation of the optimal solution for certain cases. For those cases, this is also the best achievable polynomial time approximation bound unless $P = NP$. Experiments on benchmark and real-world problems show that the proposed Quit When You Can (QWYC) algorithm can speed-up average evaluation time by $2$x--$4$x, and is around $1.5$x faster than prior work. QWYC\\'s joint optimization of ordering and thresholds also performed better in experiments than various fixed orderings, including gradient boosted trees\\' ordering.', name='arxiv', id='1de6addf-b30b-49a8-934a-c5468843a96c', tool_call_id='call_fmCrkUiYVBUHUyFQX1vDSYmv'),\n",
       " ToolMessage(content='Draft created. Draft Id: r2712269448353513460', name='create_gmail_draft', id='abeb9a88-9009-4afe-89e8-8f13209efe48', tool_call_id='call_YKNaSMgp5c9NdTfA5MZva2RK'),\n",
       " AIMessage(content='I have created a draft email with the requested subject and message to summarize the paper \"Attention is All You Need.\" Here is a brief summary of the paper for inclusion in the email:\\n\\n---\\n\\n**Title:** Attention Is All You Need\\n\\n**Summary:**\\n\\nThis paper introduces the Transformer, a novel neural network architecture based on a self-attention mechanism that provides significant improvements in tasks like translation and text generation. The paper details the operation of the attention mechanism and explains how the architecture outperforms previous models.\\n\\n---\\n\\nThe email draft has been successfully created and is ready to be sent to leehnjee@gmail.com. If you would like to send the email or require any further modifications, please let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 143, 'prompt_tokens': 1740, 'total_tokens': 1883, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DClZnNgNcuFvEyKnXQq9fosGChRZ4', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c8f91-4bf1-7ea1-863a-6dad663e76a0-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1740, 'output_tokens': 143, 'total_tokens': 1883, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='초안에 논문의 출처 url도 추가해주세요.', additional_kwargs={}, response_metadata={}, id='6bf04d08-99cd-48e2-9831-20a3df022dc1'),\n",
       " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 1902, 'total_tokens': 1925, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1792}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DCla2qioAfy8SL9TNLr9t11wTLoTT', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c8f91-85f6-7400-8439-2914cf4bf102-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'Attention is All You Need site:arxiv.org'}, 'id': 'call_jfo22Ohn8k6ztv5m2o6Vkf0K', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 1902, 'output_tokens': 23, 'total_tokens': 1925, 'input_token_details': {'audio': 0, 'cache_read': 1792}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='Published: 2021-05-06\\nTitle: Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet\\nAuthors: Luke Melas-Kyriazi\\nSummary: The strong performance of vision transformers on image classification and other vision tasks is often attributed to the design of their multi-head attention layers. However, the extent to which attention is responsible for this strong performance remains unclear. In this short report, we ask: is the attention layer even necessary? Specifically, we replace the attention layer in a vision transformer with a feed-forward layer applied over the patch dimension. The resulting architecture is simply a series of feed-forward layers applied over the patch and feature dimensions in an alternating fashion. In experiments on ImageNet, this architecture performs surprisingly well: a ViT/DeiT-base-sized model obtains 74.9\\\\% top-1 accuracy, compared to 77.9\\\\% and 79.9\\\\% for ViT and DeiT respectively. These results indicate that aspects of vision transformers other than attention, such as the patch embedding, may be more responsible for their strong performance than previously thought. We hope these results prompt the community to spend more time trying to understand why our current models are as effective as they are.\\n\\nPublished: 2025-12-03\\nTitle: \"All You Need\" is Not All You Need for a Paper Title: On the Origins of a Scientific Meme\\nAuthors: Anton Alyakin\\nSummary: The 2017 paper \\'\\'Attention Is All You Need\\'\\' introduced the Transformer architecture-and inadvertently spawned one of machine learning\\'s most persistent naming conventions. We analyze 717 arXiv preprints containing \\'\\'All You Need\\'\\' in their titles (2009-2025), finding exponential growth ($R^2$ > 0.994) following the original paper, with 200 titles in 2025 alone. Among papers following the canonical \\'\\'X [Is] All You Need\\'\\' structure, \\'\\'Attention\\'\\' remains the most frequently claimed necessity (28 occurrences). Situating this phenomenon within memetic theory, we argue the pattern\\'s success reflects competitive pressures in scientific communication that increasingly favor memorability over precision. Whether this trend represents harmless academic whimsy or symptomatic sensationalism, we leave-with appropriate self-awareness-to the reader.\\n\\nPublished: 2018-06-28\\nTitle: Quit When You Can: Efficient Evaluation of Ensembles with Ordering Optimization\\nAuthors: Serena Wang, Maya Gupta, Seungil You\\nSummary: Given a classifier ensemble and a set of examples to be classified, many examples may be confidently and accurately classified after only a subset of the base models in the ensemble are evaluated. This can reduce both mean latency and CPU while maintaining the high accuracy of the original ensemble. To achieve such gains, we propose jointly optimizing a fixed evaluation order of the base models and early-stopping thresholds. Our proposed objective is a combinatorial optimization problem, but we provide a greedy algorithm that achieves a 4-approximation of the optimal solution for certain cases. For those cases, this is also the best achievable polynomial time approximation bound unless $P = NP$. Experiments on benchmark and real-world problems show that the proposed Quit When You Can (QWYC) algorithm can speed-up average evaluation time by $2$x--$4$x, and is around $1.5$x faster than prior work. QWYC\\'s joint optimization of ordering and thresholds also performed better in experiments than various fixed orderings, including gradient boosted trees\\' ordering.', name='arxiv', id='24c4e073-21d5-45d7-ac84-d295ece62346', tool_call_id='call_jfo22Ohn8k6ztv5m2o6Vkf0K'),\n",
       " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 2644, 'total_tokens': 2668, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1920}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DCla6ureeuGJ28rr9gt7UfNBzdbna', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c8f91-966b-77e2-86c1-b9e8f677c990-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'Attention is All You Need arxiv link'}, 'id': 'call_2upxMnem4h7xTziR86GvowuB', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 2644, 'output_tokens': 24, 'total_tokens': 2668, 'input_token_details': {'audio': 0, 'cache_read': 1920}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='2025. 10. 21. · Abstract:We demonstrate complete functional segregation in hybrid SSM-Transformer architectures: retrieval depends exclusively on self-attention layers. 2025. 5. 29. · tuning을 하지않았음에도 좋은 성능을 확인. 참고 링크. Vaswani, A., et al. (2017). Attention Is All You Need. arXiv:1706.03762. https://arxiv.org/abs ... 2025. 4. 5. · \"Attention Is All You Need\"는 Transformer 모델을 제안한 기념비적인 논문으로, 자연어 처리 분야에 혁신을 일으킨 연구이다. 기존의 순환 신경망(RNN) 기반 모델들과 ... 2025. 8. 3. · ATTENTION IS ALL YOU NEED Original Paper by Ashish Vaswani, Noam Shazeer ... Original Paper link- https://arxiv.org/pdf/1706.03762. Table of Contents. 1 ... 2025. 12. 12. · Abstract page for arXiv paper 2512.11254: FAIR: Focused Attention Is All You Need for Generative Recommendation.', name='duckduckgo_search', id='19f3780c-936b-4ff0-8885-9f22603e743a', tool_call_id='call_2upxMnem4h7xTziR86GvowuB'),\n",
       " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 175, 'prompt_tokens': 2930, 'total_tokens': 3105, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2560}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DCla95yYjwcIw8KhgZzgyf9H7LwKV', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c8f91-a382-7e51-a18f-48a5ec65b94e-0', tool_calls=[{'name': 'create_gmail_draft', 'args': {'message': \"이 이메일은 NLP 분야에서 Transformer 구조의 기초를 설명한 논문 'Attention is All You Need'의 요약을 포함하고 있습니다. Transformer는 주로 셀프 어텐션 메커니즘을 활용하여 번역, 텍스트 생성 등의 작업에서 혁신적인 성과를 보여주었습니다. 이 논문은 구체적으로 어텐션 메커니즘이 어떠한 방식으로 작동하는지, 그리고 기존 구조들 대비 어떠한 이점이 있는지를 설명합니다.\\n\\n논문의 출처: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\", 'to': ['leehnjee@gmail.com'], 'subject': 'Attention is All You Need 논문 요약'}, 'id': 'call_RsNfQeNyV4abOZsakdH2udq9', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 2930, 'output_tokens': 175, 'total_tokens': 3105, 'input_token_details': {'audio': 0, 'cache_read': 2560}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='Draft created. Draft Id: r-3203269290635407294', name='create_gmail_draft', id='6dfbc121-f496-4e10-a44f-a05bf2402978', tool_call_id='call_RsNfQeNyV4abOZsakdH2udq9'),\n",
       " AIMessage(content='I\\'ve added the source URL of the paper \"Attention is All You Need\" to the email draft. Here is the updated summary with the link included:\\n\\n---\\n\\n**Summary:**\\n\\nThis email includes a summary of the paper \\'Attention is All You Need,\\' which lays the foundation for the Transformer architecture in the field of NLP. The Transformer utilizes a self-attention mechanism to achieve groundbreaking results in translation, text generation, and more. The paper explains in detail how the attention mechanism works and highlights the advantages over previous architectures.\\n\\n**Paper Source:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\\n\\n---\\n\\nThe draft is ready to be sent to leehnjee@gmail.com. Let me know if you need further adjustments or if you\\'re ready to send the email!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 162, 'prompt_tokens': 3131, 'total_tokens': 3293, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 3072}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DClaBy92u9H665F9lKue3hi7QfkmG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c8f91-ae8d-7692-874e-4ce423ca7527-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 3131, 'output_tokens': 162, 'total_tokens': 3293, 'input_token_details': {'audio': 0, 'cache_read': 3072}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " HumanMessage(content='좋습니다. 이 초안을 전송해주세요.', additional_kwargs={}, response_metadata={}, id='fd785db4-6da3-4936-8cbf-21d050205725'),\n",
       " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 3310, 'total_tokens': 3484, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 3200}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DClah78QOYwEisExFjDHEoe5hG05b', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c8f92-29c4-7b93-b558-48eabe745bcb-0', tool_calls=[{'name': 'send_gmail_message', 'args': {'message': \"이 이메일은 NLP 분야에서 Transformer 구조의 기초를 설명한 논문 'Attention is All You Need'의 요약을 포함하고 있습니다. Transformer는 주로 셀프 어텐션 메커니즘을 활용하여 번역, 텍스트 생성 등의 작업에서 혁신적인 성과를 보여주었습니다. 이 논문은 구체적으로 어텐션 메커니즘이 어떠한 방식으로 작동하는지, 그리고 기존 구조들 대비 어떠한 이점이 있는지를 설명합니다.\\n\\n논문의 출처: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\", 'to': ['leehnjee@gmail.com'], 'subject': 'Attention is All You Need 논문 요약'}, 'id': 'call_tshfQ7XEuwQMxSCO2CfhKXqF', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 3310, 'output_tokens': 174, 'total_tokens': 3484, 'input_token_details': {'audio': 0, 'cache_read': 3200}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='Message sent. Message Id: 19c8f923cf2cf2b4', name='send_gmail_message', id='5f14d296-13c6-400e-ae62-fbbd39502a21', tool_call_id='call_tshfQ7XEuwQMxSCO2CfhKXqF'),\n",
       " AIMessage(content='The email with the summary of \"Attention is All You Need\" has been successfully sent to leehnjee@gmail.com. If there\\'s anything else you need, feel free to ask!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 3512, 'total_tokens': 3550, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 3456}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_64dfa806c7', 'id': 'chatcmpl-DClamCiVzbwpOtgebOcCDYzXz0XeG', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c8f92-3ec0-7c21-8a6c-1626826131be-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 3512, 'output_tokens': 38, 'total_tokens': 3550, 'input_token_details': {'audio': 0, 'cache_read': 3456}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_list = graph.get_state(config).values['messages']\n",
    "message_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a6061",
   "metadata": {},
   "source": [
    "### 첫 질문에 대한 그래프 흐름 정리해보기 \n",
    "1. START -> agent 노드\n",
    "    - MessageState: [HumanMessage]\n",
    "    - 행동: LLM이 \"도구가 필요해!\"라고 판단하여 tool_calls가 담긴 AIMessage 생성.\n",
    "\n",
    "2. agent -> tools 노드 \n",
    "    - **AIMessage에 tool_calls가 있으므로 tools_condition에 의해 tools 노드로 이동**\n",
    "    - MessageState: [HumanMessage, AIMessage(tool_calls)]\n",
    "    - 행동: arxiv, search_gmail 실행 후 결과값을 ToolMessage로 반환.\n",
    "\n",
    "3. tools -> agent 노드 (무조건 연결된 edge)\n",
    "    - MessageState: [HumanMessage, AIMessage, ToolMessage, ToolMessage]\n",
    "    - 행동: 도구 결과를 본 LLM이 \"오케이, 이제 초안을 만들자\" 하고 create_gmail_draft를 위한 두 번째 AIMessage 생성.\n",
    "\n",
    "4. agent -> tools -> agent (한 번 더 반복)\n",
    "    - 이번 AIMessage에도 tool_calls가 있으므로 tools_condition에 의해 tools 노드로 이동, tools 노드에서 ToolMessage 리턴\n",
    "    - 상태: 메시지 리스트가 점점 길어짐.\n",
    "    - 행동: 초안 생성 결과(ToolMessage)를 확인한 LLM이 최종 답변을 준비.\n",
    "\n",
    "5. agent -> END (tools_condition에 의해)\n",
    "    - 상태: 모든 메시지가 쌓인 상태.\n",
    "    - 행동: 모든 메시지 리스트를 검토한 LLM이 더 이상 tool_calls 없이 content만 채운 마지막 AIMessage 반환. \n",
    "        - **tools_condition은 \"더 실행할 도구 없네?\" 하고 END로 보냄.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
